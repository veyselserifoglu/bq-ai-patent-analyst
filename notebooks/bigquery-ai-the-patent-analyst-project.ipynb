{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":110281,"databundleVersionId":13391012,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Patent Insight Engine with BigQuery ML\n\n## Introduction\nPatents hold rich innovation data, but their unstructured PDFs, text, and diagrams pose analysis challenges. The **Patent Insight Engine** uses BigQuery ML to process 197 English patent PDFs, extracting insights, enabling semantic search, and generating summaries/trends for practical use.\n\n## Business Product and Users\nA scalable, SQL-driven IP tool for:\n- **Patent Analysts**: Triage filings and find prior art.\n- **Inventors/R&D**: Validate consistency and explore innovations.\n- **IP Firms**: Create summaries/reports.\n- **Strategists**: Forecast trends (e.g., med_tech, crypto).\nIt saves time and boosts decision-making.\n\n## Dataset Overview\n- **403 PDFs** (197 English, others in FR/DE) at `gs://gcs-public-data--labeled-patents/*.pdf`.\n- **Tables**: `extracted_data` (metadata), `invention_types` (labels), `figures` (91 diagram coordinates).\n- **Focus**: English PDFs for accuracy.\n- **Source**: [Labeled Patents](https://console.cloud.google.com/marketplace/product/global-patents/labeled-patents?inv=1&invt=Ab5j9A&project=bq-ai-patent-analyst&supportedpurview=organizationId,folder,project) (1TB/mo free tier).\n\n## Aim\nCreate a BigQuery ML prototype to:\n1. Extract text/diagram insights.\n2. Enable semantic search.\n3. Generate summaries/trends.\n4. Temporal Analysis: Show how similar patents evolved over time (e.g., \"Show me improvements in battery tech patents since 2010\").\n5. Patent-Specific Features: Extract claims/novelty scores\n\n## Tools and Approach\nUses:\n- **ML.GENERATE_TEXT**: Extracts PDF insights.\n- **ML.GENERATE_EMBEDDING/VECTOR_SEARCH**: Enables semantic search.\n- **AI.FORECAST**: Predicts trends.\n- **Object Tables/ObjectRef**: Integrates PDFs into SQL.\nFor scalability and efficiency.\n\n## Approaches\n- **Multimodal Pioneer**: Processes PDFs and diagrams for insights (unlocks mixed data).\n- **Semantic Detective**: Finds similar patents via search (enhances discovery).\n- **AI Architect**: Generates summaries/tables (delivers actionable content).\n\n## Notebook\n- It lives here: https://github.com/veyselserifoglu/bq-ai-patent-analyst/blob/main/notebooks/bigquery-ai-the-patent-analyst-project.ipynb","metadata":{}},{"cell_type":"code","source":"# BigQuery\nimport os\nfrom google.cloud import bigquery\nfrom kaggle_secrets import UserSecretsClient","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T17:45:31.387623Z","iopub.execute_input":"2025-08-16T17:45:31.388349Z","iopub.status.idle":"2025-08-16T17:45:45.133743Z","shell.execute_reply.started":"2025-08-16T17:45:31.388322Z","shell.execute_reply":"2025-08-16T17:45:45.132997Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Google Cloud Project Setup\n\nThis guide outlines the one-time setup required in Google Cloud and Kaggle to enable the analysis.\n\n---\n\n### 1. Google Cloud Project Configuration\n\nFirst, configure your Google Cloud project.\n\n1.  **Select or Create a Project**\n    * Ensure you have a Google Cloud project.\n    * Copy the **Project ID** (e.g., `my-project-12345`), not the project name.\n\n2.  **Enable Required APIs**\n    * In your project, enable the following two APIs:\n        * **Vertex AI API**\n        * **BigQuery Connection API**\n\n3.  **Create a Service Account for the Notebook**\n    * This service account allows the Kaggle notebook to act on your behalf.\n    * Navigate to **IAM & Admin** > **Service Accounts**.\n    * Click **+ CREATE SERVICE ACCOUNT**.\n    * Give it a name (e.g., `kaggle-runner`).\n    * Grant it these three roles:\n        * `BigQuery Admin`\n        * `Vertex AI User`\n        * `Service Usage Admin`\n    * After creating the account, go to > manage keys > create a new key. A file will be downloaded to your computer.\n\n---\n\n### 2. Kaggle Notebook Configuration\n\nNext, configure this Kaggle notebook to use your project.\n\n1.  **Add Kaggle Secrets**\n    * In the notebook editor, go to the **\"Add-ons\"** menu and select **\"Secrets\"**.\n    * Add two secrets:\n        * **`GCP_PROJECT_ID`**: Paste your Google Cloud **Project ID** here.\n        * **`GCP_SA_KEY`**: Open the downloaded JSON key file, copy its entire text content, and paste it here.\n\n---\n\n### 3. Final Permission Step (After Running Code)\n\nThe first time you run the setup cells in the notebook, a new BigQuery connection will be created. This connection has its own unique service account that needs permission to use AI models.\n\n1.  **Find the Connection Service Account**\n    * After running the setup cells, go to **BigQuery** > **External connections** in your Google Cloud project.\n    * Click on the connection named `llm-connection`.\n    * Copy its **Service Account ID** (it will look like `bqcx-...@...gserviceaccount.com`).\n\n2.  **Grant Permission**\n    * Go to the **IAM & Admin** page.\n    * Click **+ Grant Access**.\n    * Paste the connection's service account ID into the **\"New principals\"** box.\n    * Give it the single role of **`Vertex AI User`**.\n    * Click **Save**.\n\n---\n\nWith this setup complete, the notebook has secure access to your Google Cloud project and can run all subsequent analysis cells.","metadata":{}},{"cell_type":"code","source":"user_secrets = UserSecretsClient()\nproject_id = user_secrets.get_secret(\"GCP_PROJECT_ID\")\ngcp_key_json = user_secrets.get_secret(\"GCP_SA_KEY\")\nlocation = 'US'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T17:45:58.391564Z","iopub.execute_input":"2025-08-16T17:45:58.391969Z","iopub.status.idle":"2025-08-16T17:45:58.903509Z","shell.execute_reply.started":"2025-08-16T17:45:58.391952Z","shell.execute_reply":"2025-08-16T17:45:58.902856Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Write the key to a temporary file in the notebook's environment\nkey_file_path = 'gcp_key.json'\ntry:\n    with open(key_file_path, 'w') as f:\n        f.write(gcp_key_json)\n    \n    # Remove \"> /dev/null 2>&1\" to show the output.\n    # Authenticate the gcloud tool using the key file\n    !gcloud auth activate-service-account --key-file={key_file_path} > /dev/null 2>&1\n    \n    # Configure the gcloud tool to use your project\n    !gcloud config set project {project_id} > /dev/null 2>&1\n    \nfinally:\n    # Securely delete the key file immediately after use\n    if os.path.exists(key_file_path):\n        os.remove(key_file_path)\n\n# Enable the Vertex AI and BigQuery Connection APIs. Run only once Or Enable using the Cloud Interface.\n# !gcloud services enable aiplatform.googleapis.com bigqueryconnection.googleapis.com > /dev/null 2>&1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T20:03:20.188542Z","iopub.execute_input":"2025-08-16T20:03:20.190435Z","iopub.status.idle":"2025-08-16T20:03:29.569866Z","shell.execute_reply.started":"2025-08-16T20:03:20.190403Z","shell.execute_reply":"2025-08-16T20:03:29.568590Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"# This command creates the connection resource. Remove \"> /dev/null 2>&1\" to show the output.\n!bq mk --connection --location={location} --connection_type=CLOUD_RESOURCE llm-connection > /dev/null 2>&1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T12:06:02.252728Z","iopub.execute_input":"2025-08-15T12:06:02.253065Z","iopub.status.idle":"2025-08-15T12:06:04.241372Z","shell.execute_reply.started":"2025-08-15T12:06:02.253040Z","shell.execute_reply":"2025-08-15T12:06:04.239160Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# This command shows the details of your connection. Remove \"> /dev/null 2>&1\" to show the output.\n!bq show --connection --location={location} llm-connection > /dev/null 2>&1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T12:06:04.244039Z","iopub.execute_input":"2025-08-15T12:06:04.245194Z","iopub.status.idle":"2025-08-15T12:06:05.220181Z","shell.execute_reply.started":"2025-08-15T12:06:04.245054Z","shell.execute_reply":"2025-08-15T12:06:05.218194Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# BigQuery Resource Creation\n\nThis section runs three commands to create the necessary resources for our analysis inside your BigQuery project.\n\n---\n\n### 1. Create a Dataset in the Correct Region\n\nFirst, we create a new dataset named `patent_analysis` in our chosen region. This dataset acts as a container for the AI model and the object table we will create next.\n\n### 2. Create a Reference to the AI Model\n\nNext, we create a \"shortcut\" to Google's `gemini-1.5-flash` model. This command gives us an easy name, `gemini_flash_analyzer`, to use in our analysis queries.\n\n### 3. Create an Object Table for the PDFs\n\nFinally, we create an object table named `patent_documents_object_table`. This is a special \"map\" that points directly to all the raw PDF files in the public Google Cloud Storage bucket, making them ready for analysis.\n\n---","metadata":{}},{"cell_type":"code","source":"client = bigquery.Client(project=project_id, location=location)\nclient","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T17:46:26.834334Z","iopub.execute_input":"2025-08-16T17:46:26.834709Z","iopub.status.idle":"2025-08-16T17:46:26.842673Z","shell.execute_reply.started":"2025-08-16T17:46:26.834660Z","shell.execute_reply":"2025-08-16T17:46:26.841229Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"<google.cloud.bigquery.client.Client at 0x7ff9e4eb5c10>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# 1. Create the new dataset in the correct location\ncreate_dataset_query = f\"\"\"\nCREATE SCHEMA IF NOT EXISTS `{project_id}.patent_analysis`\nOPTIONS(location = '{location}');\n\"\"\"\n\nprint(f\"Creating dataset 'patent_analysis' in {location}...\")\njob = client.query(create_dataset_query)\ntry:\n    job.result()\n    print(\"✅ Dataset created successfully or already exists.\")\nexcept Exception as e:\n    print(f\"❌ FAILED to create dataset. Error:\\n\\n{e}\")\n\n\n# 2. Create the AI model reference inside the new dataset\ncreate_model_query = f\"\"\"\nCREATE OR REPLACE MODEL `{project_id}.patent_analysis.gemini_vision_analyzer`\n  REMOTE WITH CONNECTION `{location}.llm-connection`\n  OPTIONS (endpoint = 'gemini-2.5-flash');\n\"\"\"\n\nprint(\"\\nCreating the AI model reference...\")\njob = client.query(create_model_query)\ntry:\n    job.result() # This waits for the job to complete.\n    print(\"✅ SUCCESS: Model 'gemini_vision_analyzer' created successfully.\")\nexcept Exception as e:\n    print(f\"❌ FAILED: The query failed. Please share this full error message:\\n\\n{e}\")\n\n# TODO: replace the gcs files source with a variable\n# 3. Create the Object Table\n# This query creates the \"map\" to the PDF files inside the local 'patent_analysis' dataset.\nobject_table_query = f\"\"\"\nCREATE OR REPLACE EXTERNAL TABLE `{project_id}.patent_analysis.patent_documents_object_table`\nWITH CONNECTION `{location}.llm-connection`\nOPTIONS (\n    object_metadata = 'SIMPLE',\n    uris = ['gs://gcs-public-data--labeled-patents/*.pdf'] \n);\n\"\"\"\n\nprint(\"Creating the object table...\")\njob = client.query(object_table_query)\ntry:\n    job.result()\n    print(\"✅ Object table created successfully.\")\nexcept Exception as e:\n    print(f\"❌ FAILED to create the object table. Error:\\n\\n{e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T12:57:54.778150Z","iopub.execute_input":"2025-08-15T12:57:54.778563Z","iopub.status.idle":"2025-08-15T12:57:57.400188Z","shell.execute_reply.started":"2025-08-15T12:57:54.778534Z","shell.execute_reply":"2025-08-15T12:57:57.398141Z"}},"outputs":[{"name":"stdout","text":"\nCreating the AI model reference...\n✅ SUCCESS: Model 'gemini_vision_analyzer' created successfully.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"# AI-Powered Data Enrichment\n\nWith our setup complete, we now begin the core analysis. This phase will extract the data from both the text and diagrams within the patent PDFs.\n\n---\n\n### Step 1: Automated Text & Multilingual Translation\n\nIn this first step, we use our `gemini-2.5-flash` model to process all the PDF documents from our object table. The prompt instructs the AI to perform several tasks:\n\n* **Extract** key fields like the title, inventor, abstract, applicant, and relevant dates.\n* **Identify** the original language of the document.\n* **Translate** the title and abstract into English if the original is in German or French.\n\nThe query then parses the AI's structured JSON response and saves the clean, translated results into a new table named `ai_extraction_text`.","metadata":{}},{"cell_type":"code","source":"prompt_text = \"\"\"From this patent document, perform the following tasks:\n\n1.  **Extract these fields**: title, inventor, abstract, \n    the **Filed**, the **Date of Patent**, the international classification code, and the applicant.\n    \n2.  **Translate**: If the original title and abstract are in German or French, translate them into English.\n\n3.  **Identify Language**: Determine the original language of the document.\n\nReturn ONLY a valid JSON object with EXACTLY these ten keys: \n\"title_en\", \"inventor\", \"abstract_en\", \"filed\", \"date_of_patent\", \"class_international\", \"applicant\", and \"original_language\".\n\n**Formatting Rule**: For any key that has multiple values (like \"inventor\" or \"class_international\" or \"applicant\"), \ncombine them into a single string, separated by a comma and a space. For example: \"Igor Karp, Lev Stesin\".\n\nThe \"original_language\" value must be one of these three strings: 'EN', 'FR', or 'DE'.\nIf any other field is unavailable, use null as the value.\n\"\"\"\n\n# The main SQL query.\nsql_query = f\"\"\"\nCREATE OR REPLACE TABLE `{project_id}.patent_analysis.ai_text_extraction` AS (\n  WITH raw_json AS (\n      SELECT\n        uri,\n        ml_generate_text_llm_result AS llm_result\n      FROM\n        ML.GENERATE_TEXT(\n          MODEL `{project_id}.patent_analysis.gemini_vision_analyzer`,\n          TABLE `{project_id}.patent_analysis.patent_documents_object_table`,\n          STRUCT(\n            '''{prompt_text}''' AS prompt,\n            2048 AS max_output_tokens,\n            0.2 AS temperature,\n            TRUE AS flatten_json_output\n          )\n        )\n    ),\n    parsed_json AS (\n      -- Step 2: Clean and parse the JSON output.\n      SELECT\n        uri,\n        llm_result,\n        SAFE.PARSE_JSON(\n          REGEXP_REPLACE(llm_result, r'(?s)```json\\\\n(.*?)\\\\n```', r'\\\\1')\n        ) AS json_data\n      FROM\n        raw_json\n    )\n  SELECT\n    uri,\n    llm_result,\n    \n    SAFE.JSON_VALUE(json_data, '$.original_language') AS original_language,\n    SAFE.JSON_VALUE(json_data, '$.title_en') AS extracted_title_en,\n    SAFE.JSON_VALUE(json_data, '$.inventor') AS extracted_inventor,\n    SAFE.JSON_VALUE(json_data, '$.abstract_en') AS extracted_abstract_en,\n    SAFE.JSON_VALUE(json_data, '$.filed') AS filed_date,\n    SAFE.JSON_VALUE(json_data, '$.date_of_patent') AS official_patent_date,\n    SAFE.JSON_VALUE(json_data, '$.class_international') AS class_international,\n    SAFE.JSON_VALUE(json_data, '$.applicant') AS applican\n    \n  FROM\n    parsed_json\n);\n\"\"\"\n\n# Run the query to create the table.\nprint(\"Running multilingual text extraction on all 403 PDFs... This may take several minutes.\")\njob = client.query(sql_query)\n\ntry:\n    job.result()\nexcept Exception as e:\n    print(f\"❌ FAILED: The query failed. Error:\\n\\n{e}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}