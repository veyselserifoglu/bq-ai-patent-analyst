{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":110281,"databundleVersionId":13391012,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Patent Insight Engine with BigQuery ML\n\n## Introduction\nPatents hold rich innovation data, but their unstructured PDFs, text, and diagrams pose analysis challenges. The **Patent Insight Engine** uses BigQuery ML to process 197 English patent PDFs, extracting insights, enabling semantic search, and generating summaries/trends for practical use.\n\n## Business Product and Users\nA scalable, SQL-driven IP tool for:\n- **Patent Analysts**: Triage filings and find prior art.\n- **Inventors/R&D**: Validate consistency and explore innovations.\n- **IP Firms**: Create summaries/reports.\n- **Strategists**: Forecast trends (e.g., med_tech, crypto).\nIt saves time and boosts decision-making.\n\n## Dataset Overview\n- **403 PDFs** (197 English, others in FR/DE) at `gs://gcs-public-data--labeled-patents/*.pdf`.\n- **Tables**: `extracted_data` (metadata), `invention_types` (labels), `figures` (91 diagram coordinates).\n- **Focus**: English PDFs for accuracy.\n- **Source**: [Labeled Patents](https://console.cloud.google.com/marketplace/product/global-patents/labeled-patents?inv=1&invt=Ab5j9A&project=bq-ai-patent-analyst&supportedpurview=organizationId,folder,project) (1TB/mo free tier).\n\n## Aim\nCreate a BigQuery ML prototype to:\n1. Extract text/diagram insights.\n2. Enable semantic search.\n3. Generate summaries/trends.\n4. Temporal Analysis: Show how similar patents evolved over time (e.g., \"Show me improvements in battery tech patents since 2010\").\n5. Patent-Specific Features: Extract claims/novelty scores\n\n## Tools and Approach\nUses:\n- **ML.GENERATE_TEXT**: Extracts PDF insights.\n- **ML.GENERATE_EMBEDDING/VECTOR_SEARCH**: Enables semantic search.\n- **AI.FORECAST**: Predicts trends.\n- **Object Tables/ObjectRef**: Integrates PDFs into SQL.\nFor scalability and efficiency.\n\n## Approaches\n- **Multimodal Pioneer**: Processes PDFs and diagrams for insights (unlocks mixed data).\n- **Semantic Detective**: Finds similar patents via search (enhances discovery).\n- **AI Architect**: Generates summaries/tables (delivers actionable content).","metadata":{}},{"cell_type":"code","source":"# BigQuery\nimport os\nfrom google.cloud import bigquery\nfrom kaggle_secrets import UserSecretsClient","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T13:08:21.637545Z","iopub.execute_input":"2025-08-15T13:08:21.637923Z","iopub.status.idle":"2025-08-15T13:08:21.643203Z","shell.execute_reply.started":"2025-08-15T13:08:21.637897Z","shell.execute_reply":"2025-08-15T13:08:21.642226Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"# Google Cloud Project Setup\n\nThis guide outlines the one-time setup required in Google Cloud and Kaggle to enable the analysis.\n\n---\n\n### 1. Google Cloud Project Configuration\n\nFirst, configure your Google Cloud project.\n\n1.  **Select or Create a Project**\n    * Ensure you have a Google Cloud project.\n    * Copy the **Project ID** (e.g., `my-project-12345`), not the project name.\n\n2.  **Enable Required APIs**\n    * In your project, enable the following two APIs:\n        * **Vertex AI API**\n        * **BigQuery Connection API**\n\n3.  **Create a Service Account for the Notebook**\n    * This service account allows the Kaggle notebook to act on your behalf.\n    * Navigate to **IAM & Admin** > **Service Accounts**.\n    * Click **+ CREATE SERVICE ACCOUNT**.\n    * Give it a name (e.g., `kaggle-runner`).\n    * Grant it these three roles:\n        * `BigQuery Admin`\n        * `Vertex AI User`\n        * `Service Usage Admin`\n    * After creating the account, go to > manage keys > create a new key. A file will be downloaded to your computer.\n\n---\n\n### 2. Kaggle Notebook Configuration\n\nNext, configure this Kaggle notebook to use your project.\n\n1.  **Add Kaggle Secrets**\n    * In the notebook editor, go to the **\"Add-ons\"** menu and select **\"Secrets\"**.\n    * Add two secrets:\n        * **`GCP_PROJECT_ID`**: Paste your Google Cloud **Project ID** here.\n        * **`GCP_SA_KEY`**: Open the downloaded JSON key file, copy its entire text content, and paste it here.\n\n---\n\n### 3. Final Permission Step (After Running Code)\n\nThe first time you run the setup cells in the notebook, a new BigQuery connection will be created. This connection has its own unique service account that needs permission to use AI models.\n\n1.  **Find the Connection Service Account**\n    * After running the setup cells, go to **BigQuery** > **External connections** in your Google Cloud project.\n    * Click on the connection named `llm-connection`.\n    * Copy its **Service Account ID** (it will look like `bqcx-...@...gserviceaccount.com`).\n\n2.  **Grant Permission**\n    * Go to the **IAM & Admin** page.\n    * Click **+ Grant Access**.\n    * Paste the connection's service account ID into the **\"New principals\"** box.\n    * Give it the single role of **`Vertex AI User`**.\n    * Click **Save**.\n\n---\n\nWith this setup complete, the notebook has secure access to your Google Cloud project and can run all subsequent analysis cells.","metadata":{}},{"cell_type":"code","source":"user_secrets = UserSecretsClient()\nproject_id = user_secrets.get_secret(\"GCP_PROJECT_ID\")\ngcp_key_json = user_secrets.get_secret(\"GCP_SA_KEY\")\nlocation = 'US'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T13:08:28.829776Z","iopub.execute_input":"2025-08-15T13:08:28.830110Z","iopub.status.idle":"2025-08-15T13:08:29.165381Z","shell.execute_reply.started":"2025-08-15T13:08:28.830088Z","shell.execute_reply":"2025-08-15T13:08:29.163934Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# Write the key to a temporary file in the notebook's environment\nkey_file_path = 'gcp_key.json'\nwith open(key_file_path, 'w') as f:\n    f.write(gcp_key_json)\n\n# Remove \"> /dev/null 2>&1\" to show the output.\n# Authenticate the gcloud tool using the key file\n!gcloud auth activate-service-account --key-file={key_file_path} > /dev/null 2>&1\n\n# Configure the gcloud tool to use your project\n!gcloud config set project {project_id} > /dev/null 2>&1\n\n# Enable the Vertex AI and BigQuery Connection APIs\n!gcloud services enable aiplatform.googleapis.com bigqueryconnection.googleapis.com > /dev/null 2>&1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T13:08:30.412535Z","iopub.execute_input":"2025-08-15T13:08:30.412992Z","iopub.status.idle":"2025-08-15T13:08:38.921075Z","shell.execute_reply.started":"2025-08-15T13:08:30.412949Z","shell.execute_reply":"2025-08-15T13:08:38.919322Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# This command creates the connection resource. Remove \"> /dev/null 2>&1\" to show the output.\n!bq mk --connection --location={location} --connection_type=CLOUD_RESOURCE llm-connection > /dev/null 2>&1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T12:06:02.252728Z","iopub.execute_input":"2025-08-15T12:06:02.253065Z","iopub.status.idle":"2025-08-15T12:06:04.241372Z","shell.execute_reply.started":"2025-08-15T12:06:02.253040Z","shell.execute_reply":"2025-08-15T12:06:04.239160Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# This command shows the details of your connection. Remove \"> /dev/null 2>&1\" to show the output.\n!bq show --connection --location={location} llm-connection > /dev/null 2>&1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T12:06:04.244039Z","iopub.execute_input":"2025-08-15T12:06:04.245194Z","iopub.status.idle":"2025-08-15T12:06:05.220181Z","shell.execute_reply.started":"2025-08-15T12:06:04.245054Z","shell.execute_reply":"2025-08-15T12:06:05.218194Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# BigQuery Resource Creation\n\nThis section runs three commands to create the necessary resources for our analysis inside your BigQuery project.\n\n---\n\n### 1. Create a Dataset in the Correct Region\n\nFirst, we create a new dataset named `patent_analysis` in our chosen region. This dataset acts as a container for the AI model and the object table we will create next.\n\n### 2. Create a Reference to the AI Model\n\nNext, we create a \"shortcut\" to Google's `gemini-1.5-flash` model. This command gives us an easy name, `gemini_flash_analyzer`, to use in our analysis queries.\n\n### 3. Create an Object Table for the PDFs\n\nFinally, we create an object table named `patent_documents_object_table`. This is a special \"map\" that points directly to all the raw PDF files in the public Google Cloud Storage bucket, making them ready for analysis.\n\n---","metadata":{}},{"cell_type":"code","source":"client = bigquery.Client(project=project_id, location=location)\nclient","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T13:08:38.923566Z","iopub.execute_input":"2025-08-15T13:08:38.924362Z","iopub.status.idle":"2025-08-15T13:08:38.934186Z","shell.execute_reply.started":"2025-08-15T13:08:38.924248Z","shell.execute_reply":"2025-08-15T13:08:38.932862Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"<google.cloud.bigquery.client.Client at 0x7e9a4ab6e790>"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"# 1. Create the new dataset in the correct location\ncreate_dataset_query = f\"\"\"\nCREATE SCHEMA IF NOT EXISTS `{project_id}.patent_analysis`\nOPTIONS(location = '{location}');\n\"\"\"\n\nprint(f\"Creating dataset 'patent_analysis' in {location}...\")\njob = client.query(create_dataset_query)\ntry:\n    job.result()\n    print(\"✅ Dataset created successfully or already exists.\")\nexcept Exception as e:\n    print(f\"❌ FAILED to create dataset. Error:\\n\\n{e}\")\n\n\n# 2. Create the AI model reference inside the new dataset\ncreate_model_query = f\"\"\"\nCREATE OR REPLACE MODEL `{project_id}.patent_analysis.gemini_vision_analyzer`\n  REMOTE WITH CONNECTION `{location}.llm-connection`\n  OPTIONS (endpoint = 'gemini-2.5-flash');\n\"\"\"\n\nprint(\"\\nCreating the AI model reference...\")\njob = client.query(create_model_query)\ntry:\n    job.result() # This waits for the job to complete.\n    print(\"✅ SUCCESS: Model 'gemini_vision_analyzer' created successfully.\")\nexcept Exception as e:\n    print(f\"❌ FAILED: The query failed. Please share this full error message:\\n\\n{e}\")\n\n# TODO: replace the gcs files source with a variable\n# 3. Create the Object Table\n# This query creates the \"map\" to the PDF files inside the local 'patent_analysis' dataset.\nobject_table_query = f\"\"\"\nCREATE OR REPLACE EXTERNAL TABLE `{project_id}.patent_analysis.patent_documents_object_table`\nWITH CONNECTION `{location}.llm-connection`\nOPTIONS (\n    object_metadata = 'SIMPLE',\n    uris = ['gs://gcs-public-data--labeled-patents/*.pdf'] \n);\n\"\"\"\n\nprint(\"Creating the object table...\")\njob = client.query(object_table_query)\ntry:\n    job.result()\n    print(\"✅ Object table created successfully.\")\nexcept Exception as e:\n    print(f\"❌ FAILED to create the object table. Error:\\n\\n{e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T12:57:54.778150Z","iopub.execute_input":"2025-08-15T12:57:54.778563Z","iopub.status.idle":"2025-08-15T12:57:57.400188Z","shell.execute_reply.started":"2025-08-15T12:57:54.778534Z","shell.execute_reply":"2025-08-15T12:57:57.398141Z"}},"outputs":[{"name":"stdout","text":"\nCreating the AI model reference...\n✅ SUCCESS: Model 'gemini_vision_analyzer' created successfully.\n","output_type":"stream"}],"execution_count":17}]}